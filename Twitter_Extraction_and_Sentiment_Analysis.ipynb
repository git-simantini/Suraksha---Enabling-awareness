{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Twitter Data Extraction and Sentiment Analysis"
      ],
      "metadata": {
        "id": "GVSyETKoJGfm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6N5xSTwg-iJ",
        "outputId": "7322769b-4907-4287-8db6-d2f8ac30fc9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting snscrape\n",
            "  Downloading snscrape-0.4.3.20220106-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.8/dist-packages (from snscrape) (2022.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from snscrape) (3.8.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from snscrape) (4.9.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from snscrape) (2.23.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from snscrape) (4.6.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->snscrape) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->snscrape) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->snscrape) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->snscrape) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->snscrape) (1.7.1)\n",
            "Installing collected packages: snscrape\n",
            "Successfully installed snscrape-0.4.3.20220106\n",
            "Query: womens safety dwarka\n",
            "Enter the number of tweets you want to Analyze: 100\n",
            "Enter the number of days you want to Scrape Twitter for: 366\n"
          ]
        }
      ],
      "source": [
        "!pip install snscrape\n",
        "\n",
        "import datetime as dt\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "#Get user input\n",
        "query = input(\"Query: \")\n",
        "\n",
        "#As long as the query is valid (not empty or equal to '#')...\n",
        "if query != '':\n",
        "    noOfTweet = input(\"Enter the number of tweets you want to Analyze: \")\n",
        "    #if noOfTweet != '' :\n",
        "    noOfDays = input(\"Enter the number of days you want to Scrape Twitter for: \")\n",
        "    if noOfDays != '':\n",
        "    #Creating list to append tweet data\n",
        "        tweets_list = []\n",
        "        #now = dt.date.today()\n",
        "        now=pd.to_datetime(\"2012-12-31\")\n",
        "        now = now.strftime('%Y-%m-%d')\n",
        "        #yesterday = dt.date.today() - dt.timedelta(days = int(noOfDays))\n",
        "        yesterday=pd.to_datetime(now) - dt.timedelta(days = int(noOfDays))\n",
        "        yesterday = yesterday.strftime('%Y-%m-%d')\n",
        "        d=yesterday\n",
        "        for i in range(int(noOfDays)):\n",
        "            d=(pd.to_datetime(d) + dt.timedelta(days = 1)).strftime('%Y-%m-%d')\n",
        "            n=(pd.to_datetime(d) + dt.timedelta(days = 1)).strftime('%Y-%m-%d')\n",
        "            for i,tweet in enumerate(sntwitter.TwitterSearchScraper(query + ' lang:en since:' +  d + ' until:' + n + ' -filter:links -filter:replies').get_items()):\n",
        "                if i > int(noOfTweet):\n",
        "                    break\n",
        "                tweets_list.append([tweet.date.strftime(\"%Y-%m-%d\"),tweet.date.strftime(\"%H:%M:%S\"), tweet.id, tweet.content, tweet.username,'TWITTER'])\n",
        "                \n",
        "        #Creating a dataframe from the tweets list above \n",
        "        df = pd.DataFrame(tweets_list, columns=['Date','Time', 'Tweet Id', 'Text', 'Username','Source'])\n",
        "        \n",
        "df\n",
        "\n",
        "# Create a function to clean the tweets\n",
        "def cleanTxt(text):\n",
        "    text = re.sub('@[A-Za-z0–9]+', '', text) #Removing @mentions\n",
        "    text = re.sub('#', '', text) # Removing '#' hash tag\n",
        "    text = re.sub('RT[\\s]+', '', text) # Removing RT\n",
        "    text = re.sub('https?:\\/\\/\\S+', '', text) # Removing hyperlink\n",
        "    return text\n",
        "\n",
        "#applying this function to Text column of our dataframe\n",
        "df[\"Text\"] = df[\"Text\"].apply(cleanTxt)\n",
        "\n",
        "\n",
        "df= df[[\"Date\",\"Time\",\"Text\"]]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('Twitter.csv', mode='a', index = False, header=None)"
      ],
      "metadata": {
        "id": "GVMC4XSSiE-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data =pd.read_csv(\"/content/Twitter.csv\")\n",
        "data.head()\n",
        "\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "vader=SentimentIntensityAnalyzer()\n",
        "\n",
        "compond=lambda summary: vader.polarity_scores(summary)['compound']\n",
        "data['Compound']=data['SUMMARY'].apply(compond)\n",
        "\n",
        "negative=lambda summary: vader.polarity_scores(summary)['neg']\n",
        "data['Negative']=data['SUMMARY'].apply(negative)\n",
        "\n",
        "neutral=lambda summary: vader.polarity_scores(summary)['neu']\n",
        "data['Neutral']=data['SUMMARY'].apply(neutral)\n",
        "\n",
        "positive=lambda summary: vader.polarity_scores(summary)['pos']\n",
        "data['Positive']=data['SUMMARY'].apply(positive)\n",
        "data\n",
        "\n",
        "\n",
        "data.to_csv('Twitter_output.csv',mode='a',index = False, header=None)\n",
        "\n",
        "\n",
        "#Sentiment Analysis\n",
        "def percentage(part,whole):\n",
        "    return 100 * float(part)/float(whole)\n",
        "\n",
        "#Assigning Initial Values\n",
        "positive = 0\n",
        "negative = 0\n",
        "neutral = 0\n",
        "#Creating empty lists\n",
        "summary_list = []\n",
        "neutral_list = []\n",
        "negative_list = []\n",
        "positive_list = []\n",
        "\n",
        "\n",
        "#Iterating over the Summary in the dataframe\n",
        "for summary in data['SUMMARY']:\n",
        "    summary_list.append(summary)\n",
        "    analyzer = SentimentIntensityAnalyzer().polarity_scores(summary)\n",
        "    neg = analyzer['neg']\n",
        "    neu = analyzer['neu']\n",
        "    pos = analyzer['pos']\n",
        "    comp = analyzer['compound']\n",
        "    \n",
        "    if neg > pos:\n",
        "        negative_list.append(summary) #appending the tweet that satisfies this condition\n",
        "        negative += 1 #increasing the count by 1\n",
        "    elif pos > neg:\n",
        "        positive_list.append(summary) #appending the tweet that satisfies this condition\n",
        "        positive += 1 #increasing the count by 1\n",
        "    elif pos == neg:\n",
        "        neutral_list.append(summary) #appending the tweet that satisfies this condition\n",
        "        neutral += 1 #increasing the count by 1 \n",
        "\n",
        "positive = percentage(positive, len(data)) #percentage is the function defined above\n",
        "negative = percentage(negative, len(data))\n",
        "neutral = percentage(neutral, len(data))\n",
        "\n",
        "\n",
        "#Converting lists to pandas dataframe\n",
        "summary_list = pd.DataFrame(summary_list)\n",
        "neutral_list = pd.DataFrame(neutral_list)\n",
        "negative_list = pd.DataFrame(negative_list)\n",
        "positive_list = pd.DataFrame(positive_list)\n",
        "\n",
        "#using len(length) function for counting\n",
        "print(\"Last 365 days, there have been\", len(summary_list) ,  \"tweets/news on women's safety in Delhi\" ,  end='\\n*')\n",
        "print(\"Positive Sentiment:\", '%.2f' % len(positive_list), end='\\n*')\n",
        "print(\"Neutral Sentiment:\", '%.2f' % len(neutral_list), end='\\n*')\n",
        "print(\"Negative Sentiment:\", '%.2f' % len(negative_list), end='\\n*')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "XzqY2bteIm-I",
        "outputId": "5a57028c-09d3-49b2-bd1d-a754f3d10f2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'SUMMARY'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-1914be08e1b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mcompond\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'compound'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Compound'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SUMMARY'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'neg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'SUMMARY'"
          ]
        }
      ]
    }
  ]
}